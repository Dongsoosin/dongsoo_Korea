{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(pd.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if \"bert\" not in os.listdir():\n",
    "  os.makedirs(\"bert\")\n",
    "else:\n",
    "  pass\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "         \n",
    "bert_zip = zipfile.ZipFile('multi_cased_L-12_H-768_A-12.zip')\n",
    "bert_zip.extractall('bert')\n",
    " \n",
    "bert_zip.close()\n",
    "\n",
    "def copytree(src, dst, symlinks=False, ignore=None):\n",
    "    for item in os.listdir(src):\n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "copytree(\"bert/multi_cased_L-12_H-768_A-12\", \"bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp gdrive/MyDrive/korquad/KorQuAD_v1.0_dev.json ./\n",
    "%cp gdrive/MyDrive/korquad/KorQuAD_v1.0_train.json ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로우 1 기반임\n",
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras import Input, Model\n",
    "from keras import optimizers\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-transformer==0.32.0\n",
    "!pip install keras-bert==0.81.0\n",
    "!pip install keras-radam==0.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],verbose = 1):\n",
    "  \n",
    "    if verbose:\n",
    "        print(\"Json 파일 읽는 중\")    \n",
    "    file = json.loads(open(input_file_path).read())\n",
    "    if verbose:\n",
    "        print(\"데이터 정제 중\")\n",
    "    # parsing different level's in the json file\n",
    "    js = pd.io.json.json_normalize(file , record_path )\n",
    "    m = pd.io.json.json_normalize(file, record_path[:-1] )\n",
    "    r = pd.io.json.json_normalize(file,record_path[:-2])\n",
    "    \n",
    "    #combining it into single dataframe\n",
    "    idx = np.repeat(r['context'].values, r.qas.str.len())\n",
    "    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n",
    "    m['context'] = idx\n",
    "    js['q_idx'] = ndx\n",
    "    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n",
    "    main['c_id'] = main['context'].factorize()[0]\n",
    "    if verbose:\n",
    "        print(\"데이터 크기는 {}\".format(main.shape))\n",
    "        print(\"완료\")\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = squad_json_to_dataframe_train(\"./KorQuAD_v1.0_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 학습을 위한 seq_len부터 학습률 등 파라미터 정의#모델 학습을 위한 seq_len부터 학습률 등 파라미터 정의\n",
    "SEQ_LEN = 384\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS=1\n",
    "LR=3e-5\n",
    "\n",
    "#모델이 들어있는 폴더\n",
    "pretrained_path =\"bert\"\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "DATA_COLUMN = \"context\"\n",
    "QUESTION_COLUMN = \"question\"\n",
    "TEXT = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        if \"_\" in token:\n",
    "          token = token.replace(\"_\",\"\")\n",
    "          token = \"##\" + token\n",
    "        token_dict[token] = len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class inherit_Tokenizer(Tokenizer):\n",
    "  def _tokenize(self, text):\n",
    "        if not self._cased:\n",
    "            text = text\n",
    "            \n",
    "            text = text.lower()\n",
    "        spaced = ''\n",
    "        for ch in text:\n",
    "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
    "                spaced += ' ' + ch + ' '\n",
    "            elif self._is_space(ch):\n",
    "                spaced += ' '\n",
    "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
    "                continue\n",
    "            else:\n",
    "                spaced += ch\n",
    "        tokens = []\n",
    "        for word in spaced.strip().split():\n",
    "            tokens += self._word_piece_tokenize(word)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = inherit_Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.encode('안녕하세요. 인코딩 테스트입니다.'))\n",
    "print(tokenizer.tokenize(\"안녕하세요. 인코딩 테스트입니다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_token_dict = {v : k for k, v in token_dict.items()}\n",
    "list(reverse_token_dict.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = train['question'][0]\n",
    "context = train['context'][0]\n",
    "text = train['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question,'\\n',context,'\\n',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.tokenize(question, context))\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "    indices, segments, target_start, target_end = [], [], [], []\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        \n",
    "        ids, segment = tokenizer.encode(data_df[QUESTION_COLUMN][i], data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n",
    "        \n",
    "\n",
    "        text = tokenizer.encode(data_df[TEXT][i])[0]\n",
    "\n",
    "        text_slide_len = len(text[1:-1])\n",
    "        for i in range(1,len(ids)-text_slide_len-1):  \n",
    "            exist_flag = 0\n",
    "            if text[1:-1] == ids[i:i+text_slide_len]:\n",
    "              ans_start = i\n",
    "              ans_end = i + text_slide_len - 1\n",
    "              exist_flag = 1\n",
    "              break\n",
    "        \n",
    "        if exist_flag == 0:\n",
    "          ans_start = SEQ_LEN\n",
    "          ans_end = SEQ_LEN\n",
    "\n",
    "        indices.append(ids)\n",
    "        segments.append(segment)\n",
    "\n",
    "        target_start.append(ans_start)\n",
    "        target_end.append(ans_end)\n",
    "\n",
    "    indices_x = np.array(indices)\n",
    "    segments = np.array(segments)\n",
    "    target_start = np.array(target_start)\n",
    "    target_end = np.array(target_end)\n",
    "    \n",
    "    del_list = np.where(target_start!=SEQ_LEN)[0]\n",
    "\n",
    "    indices_x = indices_x[del_list]\n",
    "    segments = segments[del_list]\n",
    "    target_start = target_start[del_list]\n",
    "    target_end = target_end[del_list]\n",
    "\n",
    "    train_y_0 = keras.utils.to_categorical(target_start, num_classes=SEQ_LEN, dtype='int64')\n",
    "    train_y_1 = keras.utils.to_categorical(target_end, num_classes=SEQ_LEN, dtype='int64')\n",
    "    train_y_cat = [train_y_0, train_y_1]\n",
    "    \n",
    "    return [indices_x, segments], train_y_cat\n",
    "\n",
    "def load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    \n",
    "    \n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "    data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)\n",
    "\n",
    "\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = load_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_num = 12\n",
    "model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    training=False,\n",
    "    trainable=True,\n",
    "    seq_len=SEQ_LEN,)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonMasking(Layer):   \n",
    "    def __init__(self, **kwargs):   \n",
    "        self.supports_masking = True  \n",
    "        super(NonMasking, self).__init__(**kwargs)   \n",
    "  \n",
    "    def build(self, input_shape):   \n",
    "        input_shape = input_shape   \n",
    "  \n",
    "    def compute_mask(self, input, input_mask=None):   \n",
    "        return None   \n",
    "  \n",
    "    def call(self, x, mask=None):   \n",
    "        return x   \n",
    "  \n",
    "    def get_output_shape_for(self, input_shape):   \n",
    "        return input_shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer_Start(Layer):\n",
    "\n",
    "    def __init__(self,seq_len, **kwargs):\n",
    "        self.seq_len = seq_len\n",
    "        self.supports_masking = True\n",
    "        super(MyLayer_Start, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                 shape=(input_shape[2],2),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(MyLayer_Start, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        \n",
    "        x = K.reshape(x, shape=(-1,self.seq_len,K.shape(x)[2]))\n",
    "        x = K.dot(x, self.W)\n",
    "        x = K.permute_dimensions(x, (2,0,1))\n",
    "        self.start_logits, self.end_logits = x[0], x[1]  \n",
    "        self.start_logits = K.softmax(self.start_logits, axis=-1)\n",
    "        return self.start_logits\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.seq_len)\n",
    "\n",
    "class MyLayer_End(Layer):\n",
    "  def __init__(self,seq_len, **kwargs):\n",
    "        self.seq_len = seq_len\n",
    "        self.supports_masking = True\n",
    "        super(MyLayer_End, self).__init__(**kwargs)\n",
    "  \n",
    "  def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='kernel', \n",
    "                                 shape=(input_shape[2], 2),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(MyLayer_End, self).build(input_shape)\n",
    "  \n",
    "  def call(self, x):\n",
    "        x = K.reshape(x, shape=(-1,self.seq_len,K.shape(x)[2]))\n",
    "        x = K.dot(x, self.W)\n",
    "        x = K.permute_dimensions(x, (2,0,1))\n",
    "        self.start_logits, self.end_logits = x[0], x[1]\n",
    "        self.end_logits = K.softmax(self.end_logits, axis=-1)\n",
    "        return self.end_logits\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import merge, dot, concatenate\n",
    "from keras import metrics\n",
    "def get_bert_finetuning_model(model):\n",
    "  inputs = model.inputs[:2]\n",
    "  dense = model.output\n",
    "  x = NonMasking()(dense)\n",
    "  outputs_start = MyLayer_Start(SEQ_LEN)(x)\n",
    "  outputs_end = MyLayer_End(SEQ_LEN)(x)\n",
    "  bert_model = keras.models.Model(inputs, [outputs_start, outputs_end])\n",
    "  bert_model.compile(\n",
    "      optimizer=RAdam(learning_rate=LR, decay=0.0001),\n",
    "      loss='categorical_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "  \n",
    "  return bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "\n",
    "\n",
    "SVG(model_to_dot(get_bert_finetuning_model(model), dpi=65).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n",
    "sess.run(init)\n",
    "\n",
    "bert_model = get_bert_finetuning_model(model)\n",
    "bert_model.summary()\n",
    "history = bert_model.fit(train_x, train_y, batch_size=128, validation_split=0.05, shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = get_bert_finetuning_model(model)\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"gdrive/MyDrive/korquad/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_weights(path+\"/squad_wordpiece.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.compile(optimizer=RAdam(learning_rate=0.00003, decay=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "bert_model.fit(train_x, train_y, batch_size=10, shuffle=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.save_weights(path+\"/squad_wordpiece_2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = get_bert_finetuning_model(model)\n",
    "bert_model.load_weights(path+\"/korquad_wordpiece_2.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
